{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericgao96/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions/blob/master/Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e03b2a66",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-03-18T01:15:04.703766Z",
          "start_time": "2022-03-18T01:15:03.222293Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e03b2a66",
        "outputId": "2a7541c0-e553-4c7a-eeb7-cc09f9fae09c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ribs[all]\n",
            "  Downloading ribs-0.4.0-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym~=0.17.0 in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting Box2D~=2.3.10\n",
            "  Downloading Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.21.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym~=0.17.0) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym~=0.17.0) (0.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (3.1.0)\n",
            "Requirement already satisfied: decorator>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (4.4.2)\n",
            "Collecting toml>=0.10.0\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (0.51.2)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (2.4.0)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (1.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs[all]) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ribs[all]) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs[all]) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs[all]) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->ribs[all]) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->ribs[all]) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ribs[all]) (1.1.0)\n",
            "Installing collected packages: toml, ribs, Box2D\n",
            "Successfully installed Box2D-2.3.10 ribs-0.4.0 toml-0.10.2\n"
          ]
        }
      ],
      "source": [
        "%pip install ribs[all] gym~=0.17.0 Box2D~=2.3.10 tqdm\n",
        "import gym\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12388c6a",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2022-03-18T01:15:03.220Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12388c6a",
        "outputId": "9fbd75c2-5b36-4732-9a61-2c37c1399615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "env.seed(1)\n",
        "N_EPISODE = 1000\n",
        "LR = 1e-3 # LR = 1E-4, EPSILON_END = 1E-2, BATCH_SIZE = 32 would work\n",
        "GAMMA = 0.99\n",
        "EPSILON = 1\n",
        "EPSILON_END = 0.01\n",
        "MEMORY_CAPACITY = 100000\n",
        "BATCH_SIZE = 32\n",
        "N_ACTION = env.action_space.n\n",
        "N_STATE_VAR = env.observation_space.shape[0]\n",
        "TARGET_UPDATE_FREQ = 10\n",
        "EPSILON_DECAY = 0.999\n",
        "LAYER1_NEURON = 128\n",
        "LAYER2_NEURON = 64\n",
        "\n",
        "\n",
        "# Get cpu or gpu device for training.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "def creat_nn_model(layer1_neuron=128, layer2_neuron=64, device=device):\n",
        "  net = NeuralNetwork(layer1_neuron, layer2_neuron).to(device)\n",
        "  return net\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, layer1_neuron, layer2_neuron):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(N_STATE_VAR, layer1_neuron),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(layer1_neuron, layer2_neuron),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(layer2_neuron, N_ACTION)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        actions_value = self.linear_relu_stack(x)\n",
        "        return actions_value\n",
        "\n",
        "D = np.zeros((MEMORY_CAPACITY, N_STATE_VAR * 2 + 3)) # store S, action, reward, is_terminal, and S_prime in replay memory\n",
        "param_update_counter = 0\n",
        "memory_counter = 0\n",
        "epsilon = EPSILON\n",
        "reward_list = []\n",
        "\n",
        "policy_net = creat_nn_model(LAYER1_NEURON, LAYER2_NEURON, device)\n",
        "target_net = creat_nn_model(LAYER1_NEURON, LAYER2_NEURON, device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(policy_net.parameters(), lr=LR)\n",
        "\n",
        "for episode in range(N_EPISODE):\n",
        "    S = env.reset()\n",
        "    reward_episode = 0\n",
        "    for step in range(1000):\n",
        "        # epsilon-greedy policy\n",
        "        is_greedy = np.random.random() > epsilon\n",
        "        if is_greedy:\n",
        "            action = torch.argmax(policy_net(torch.tensor(S.reshape(1,-1), device=device, dtype=torch.float))).item()\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        S_prime, reward, done, info = env.step(action)\n",
        "        # store experience in replay memory\n",
        "        store_index = memory_counter % MEMORY_CAPACITY\n",
        "        D[store_index] = np.concatenate([S,[action, reward, done], S_prime])\n",
        "        memory_counter += 1\n",
        "        S = S_prime\n",
        "        reward_episode += reward\n",
        "        \n",
        "        # train only when have more than BATCH_SIZE samples\n",
        "        if memory_counter >= BATCH_SIZE:\n",
        "            # sample random batch of expereince from memory\n",
        "            sample_index = np.random.choice(min(memory_counter, MEMORY_CAPACITY), size=BATCH_SIZE, replace=False)\n",
        "            sample_memory = D[sample_index]\n",
        "            S_memory = sample_memory[:, :N_STATE_VAR]\n",
        "            S_prime_memory = sample_memory[:, -N_STATE_VAR:]\n",
        "            action_memory = sample_memory[:, N_STATE_VAR, None]\n",
        "            reward_memory = sample_memory[:, N_STATE_VAR+1, None]\n",
        "            done_memory = sample_memory[:, N_STATE_VAR+2, None]\n",
        "            \n",
        "            batch_S = torch.tensor(S_memory, device=device, dtype=torch.float)\n",
        "            batch_S_prime = torch.tensor(S_prime_memory, device=device, dtype=torch.float)\n",
        "            batch_action = torch.tensor(action_memory.astype(int), device=device, dtype=torch.long)\n",
        "            batch_reward = torch.tensor(reward_memory, device=device, dtype=torch.float)\n",
        "            batch_done = torch.tensor(done_memory.astype(int), device=device, dtype=torch.float)\n",
        "            \n",
        "            \n",
        "            # set training target\n",
        "            Q_next = target_net(batch_S_prime).detach()\n",
        "            Q_pred = policy_net(batch_S).gather(1, batch_action)\n",
        "            Q_target = (batch_reward + GAMMA * Q_next.max(1)[0].reshape(-1,1))\n",
        "            Q_target = torch.where(batch_done == 1, batch_reward, Q_target)\n",
        "            \n",
        "            # perform gradient descent to update weights\n",
        "            loss = loss_fn(Q_pred, Q_target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "#             for param in policy_net.parameters():\n",
        "#                 param.grad.data.clamp_(-1, 1)\n",
        "            optimizer.step()\n",
        "            \n",
        "            # update target network params\n",
        "            if param_update_counter % TARGET_UPDATE_FREQ == 0:\n",
        "                target_net.load_state_dict(policy_net.state_dict())\n",
        "            param_update_counter += 1\n",
        "                \n",
        "        if done or step == 999:\n",
        "            reward_list.append(reward_episode)\n",
        "            break\n",
        "            \n",
        "        # epsilon decay\n",
        "        if epsilon > EPSILON_END:\n",
        "            epsilon = epsilon * EPSILON_DECAY\n",
        "    if episode % 100 == 0 and episode != 0:\n",
        "        print('Episode: {episode}, Reward: {reward:.2f}, Average Reward: {avg_reward:.2f}'.format(\n",
        "            episode=episode, reward=reward_episode, avg_reward=np.mean(reward_list[-100:])))"
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Project2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}